{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYCFhSKN1U8f"
      },
      "source": [
        "# Data loading and initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S2KYuyn5-wdy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Xception or VGG19\n",
        "APPLICATION                   = 'VGG19'\n",
        "\n",
        "# to be modified as required in the execution\n",
        "# I used 0 and 123 to test my results\n",
        "RANDOM_STATE                  = 0\n",
        "\n",
        "USE_GOOGLE_DRIVE_FOR_FILES    = True\n",
        "DATA_FOLDER_PATH              = \"/content/drive/My Drive/Data Science/BGU/Machine Learning/Assignments/3/Data\"\n",
        "\n",
        "# name of the folder in the data folder, in which the images are stored.\n",
        "# when using googel drive, a folder with the same name will be created locally.\n",
        "# all images are in the format image_<5 chars number>.jpg (e.g: image_00123.jpg)\n",
        "IMAGES_FOLDER_NAME            = '102-jpg'\n",
        "\n",
        "# for debug only. leave empty if not in debug mode.\n",
        "# in debug mode the results will be saved to SAVE_PATH.\n",
        "RUN_NUMBER                    = \"1007\"\n",
        "SAVE_PATH                     = \"/content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htXPO-fVqr0w",
        "outputId": "fd671fa5-96dd-44c6-f9e5-97937860a919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing           import image\n",
        "from tensorflow.keras.optimizers              import Adam\n",
        "from keras                                    import Input, Sequential\n",
        "from keras.applications.vgg19                 import VGG19, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications.xception   import Xception\n",
        "from keras.callbacks                          import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "from keras.layers                             import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout, Activation\n",
        "from keras.models                             import Model, load_model\n",
        "from sklearn.preprocessing                    import label_binarize\n",
        "from sklearn.model_selection                  import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "if APPLICATION == 'Xception':\n",
        "  image_size = 299\n",
        "elif APPLICATION == 'VGG19':\n",
        "  # original is 224, but I get a higher accuracy with 299\n",
        "  image_size = 299\n",
        "else:\n",
        "  raise ValueError(f'APPLICATION {APPLICATION} undefined')\n",
        "\n",
        "if USE_GOOGLE_DRIVE_FOR_FILES:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "# Load labels\n",
        "df_y = pd.read_csv(f\"{DATA_FOLDER_PATH}/102Flowers_labels.csv\")\n",
        "\n",
        "# Load class dictionary\n",
        "with open(f\"{DATA_FOLDER_PATH}/classes_dictionary.txt\", 'r') as f:\n",
        "  classes_dictionary_str = json.loads(f.read())\n",
        "\n",
        "# convert classes from str to int\n",
        "classes_dictionary = {}\n",
        "for k in classes_dictionary_str.keys():\n",
        "  classes_dictionary[int(k)] = classes_dictionary_str[k]\n",
        "\n",
        "# load the files locally\n",
        "if USE_GOOGLE_DRIVE_FOR_FILES and not os.path.exists(IMAGES_FOLDER_NAME):\n",
        "  os.mkdir(IMAGES_FOLDER_NAME)\n",
        "\n",
        "if RUN_NUMBER:\n",
        "  \n",
        "  save_results = True\n",
        "\n",
        "  # folder to store files\n",
        "  if not os.path.exists(f'{SAVE_PATH}/{RUN_NUMBER}'):\n",
        "    os.mkdir(f\"{SAVE_PATH}/{RUN_NUMBER}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if USE_GOOGLE_DRIVE_FOR_FILES:\n",
        "  print(\"Loading images from drive and saving locally...\")\n",
        "  i=0\n",
        "  for filename in os.listdir(f\"{DATA_FOLDER_PATH}/{IMAGES_FOLDER_NAME}\"):\n",
        "    with open(f\"{DATA_FOLDER_PATH}/{IMAGES_FOLDER_NAME}/\"+filename, 'rb') as f:\n",
        "      with open (f'{IMAGES_FOLDER_NAME}/{filename}', 'wb') as fw:\n",
        "        fw.write(f.read())\n",
        "        i+=1\n",
        "        if i%1000 == 0:\n",
        "          print(i, end=', ')\n",
        "  print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PbLN4tWgSKu",
        "outputId": "77f41a3e-2f8a-4153-cf63-bb43d735c0ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from drive and saving locally...\n",
            "1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 8189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BlBtqp2Xk4q",
        "outputId": "ed29b8d4-c9fa-40b3-c4fa-1be221babc11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images to variable X...\n",
            "0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 8188\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Loading images to variable X...\")\n",
        "X = []\n",
        "for i in range(len(df_y)):\n",
        "  img = image.load_img(f'{IMAGES_FOLDER_NAME}/image_{str(i+1).zfill(5)}.jpg', target_size=(image_size,image_size))\n",
        "  X.append(image.img_to_array(img))\n",
        "  if i%1000 == 0:\n",
        "    print(i, end=', ')\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For traceability and logging, the IDs of the test will be saved.\n",
        "# Therefore the train/validation/test splits will be done on the IDs, not the data itself.\n",
        "\n",
        "X_ids_for_split = pd.DataFrame({'ID':np.arange(len(X))})\n",
        "y = df_y['y']\n",
        "\n",
        "# wrong label in the data\n",
        "y[7592]=96\n",
        "\n",
        "# Split: 50% train, 25% validation, 25% test\n",
        "\n",
        "X_temp, X_test_ids_for_split, y_temp, y_test = train_test_split(\n",
        "    X_ids_for_split, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "X_train_ids_for_split, X_valid_ids_for_split, y_train, y_valid = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.3333, random_state=RANDOM_STATE, stratify=y_temp)\n",
        "\n",
        "\n",
        "X_test = [X[i] for i in list(X_test_ids_for_split['ID'])]\n",
        "X_valid = [X[i] for i in list(X_valid_ids_for_split['ID'])]\n",
        "X_train = [X[i] for i in list(X_train_ids_for_split['ID'])]\n",
        "\n",
        "# save test IDs for debug\n",
        "if save_results:\n",
        "  X_test_ids_for_split.to_csv(f'{SAVE_PATH}/{RUN_NUMBER}/X_test_IDs.csv')\n",
        "\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "X_valid = np.array(X_valid)\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "classes = list(classes_dictionary.keys())\n",
        "classes.sort()\n",
        "\n",
        "y_test = label_binarize(np.array(y_test), classes = classes)\n",
        "y_valid = label_binarize(np.array(y_valid), classes = classes)\n",
        "y_train = label_binarize(np.array(y_train), classes = classes)\n"
      ],
      "metadata": {
        "id": "nVL7coZCoWud"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# free up memory\n",
        "del X\n",
        "del X_temp\n",
        "del X_ids_for_split\n",
        "del X_train_ids_for_split\n",
        "del X_valid_ids_for_split\n",
        "del X_test_ids_for_split\n",
        "del df_y\n",
        "del y_temp\n",
        "del y\n"
      ],
      "metadata": {
        "id": "gh0WdrhP7bJv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ4gq58G1ppu"
      },
      "source": [
        "# Transfer learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eRm57T2pNPzg"
      },
      "outputs": [],
      "source": [
        "\n",
        "if APPLICATION == 'Xception':\n",
        "\n",
        "  # Xception weights requires that input be scaled from (0, 255) to a range of (-1., +1.)\n",
        "  X_test = X_test / 127.5 - 1.\n",
        "  X_valid = X_valid / 127.5 - 1.\n",
        "  X_train = X_train / 127.5 - 1.\n",
        "\n",
        "elif APPLICATION == 'VGG19':\n",
        "\n",
        "  # subtracting the mean RGB value, computed on the training set, from each pixel\n",
        "  X_test = preprocess_input(X_test)\n",
        "  X_valid = preprocess_input(X_valid)\n",
        "  X_train = preprocess_input(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# short run params\n",
        "\n",
        "# num_epochs = 30\n",
        "# learning_rate = 1 * 1e-3\n",
        "# early_stopping_patience = 4\n",
        "# early_stopping_min_delta = 0.003\n",
        "\n",
        "# long run params\n",
        "num_epochs = 120\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = Adam(learning_rate=learning_rate) \n",
        "\n",
        "model_file_prefix = f'{SAVE_PATH}/{RUN_NUMBER}/{RANDOM_STATE}' \n",
        "\n",
        "# callback 1: early stopping (for short runs)\n",
        "#es = EarlyStopping(monitor='val_loss', patience=early_stopping_patience, min_delta=early_stopping_min_delta, verbose=1)\n",
        "\n",
        "# callback 2: model checkpoint\n",
        "cp = ModelCheckpoint(filepath=model_file_prefix + '_best_model.h5', mode='max', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "# callback 3: reduce learning rate on plateau. all values are default except verbose.\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                              factor=0.1,\n",
        "                              patience=10,\n",
        "                              verbose=1,\n",
        "                              mode=\"auto\",\n",
        "                              min_delta=0.0001,\n",
        "                              cooldown=0,\n",
        "                              min_lr=0\n",
        "                              )\n",
        "\n",
        "# For fitting few times in a sequence, use validation_data = (X_valid, y_valid), otherwise use validation_split. Here I use it like that for control and debugging\n",
        "validation_data = (X_valid, y_valid)\n",
        "\n",
        "# include_top=False --> get the outputs of the convolution layers, before the FC layers\n",
        "if APPLICATION == 'Xception':\n",
        "\n",
        "  base_model = Xception(input_shape=[image_size, image_size, 3], weights='imagenet', include_top=False)\n",
        "\n",
        "  base_model.trainable = False\n",
        "\n",
        "  inputs = Input(shape=(image_size, image_size, 3))\n",
        "\n",
        "  o = base_model(inputs, training=False)\n",
        "  o = GlobalAveragePooling2D()(o)\n",
        "  o = Dense(512)(o)\n",
        "  o = BatchNormalization()(o)\n",
        "  o = Activation('relu')(o)\n",
        "  o = Dropout(rate=0.3, seed=RANDOM_STATE)(o)\n",
        "\n",
        "elif APPLICATION == 'VGG19':\n",
        "\n",
        "  base_model = VGG19(input_shape=[image_size, image_size, 3], weights='imagenet', include_top=False)\n",
        "\n",
        "  base_model.trainable = False\n",
        "\n",
        "  inputs = Input(shape=(image_size, image_size, 3))\n",
        "\n",
        "  o = base_model(inputs, training=False)\n",
        "  o = GlobalAveragePooling2D()(o)\n",
        "  o = Dense(1024)(o)\n",
        "  o = BatchNormalization()(o)\n",
        "  o = Activation('relu')(o)\n",
        "  o = Dropout(rate=0.3, seed=RANDOM_STATE)(o)\n",
        "  \n",
        "\n",
        "\n",
        "predictions = Dense(len(classes_dictionary), activation='softmax')(o)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "model.compile(optimizer=optimizer,  loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "crRoSWpJBGMs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# free up some memory\n",
        "del X_valid\n",
        "del y_valid"
      ],
      "metadata": {
        "id": "mRSCMytz7CSm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_test={'test_loss': [], 'test_accuracy': []}\n",
        "\n",
        "class EvaluateEpochEnd(Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        #super().on_epoch_end(epoch, logs)\n",
        "        x, y = self.test_data\n",
        "        scores = self.model.evaluate(x, y, verbose=False)\n",
        "        history_test['test_loss'].append(scores[0])\n",
        "        history_test['test_accuracy'].append(scores[1])\n",
        "        print('\\nTesting loss: {}, accuracy: {}\\n'.format(scores[0], scores[1]))\n",
        "\n",
        "class EpochModelCheckpoint(ModelCheckpoint):\n",
        "\n",
        "    def __init__(self,\n",
        "                 filepath,\n",
        "                 frequency=1,\n",
        "                 monitor='val_loss',\n",
        "                 verbose=0,\n",
        "                 save_best_only=False,\n",
        "                 save_weights_only=False,\n",
        "                 mode='auto',\n",
        "                 options=None,\n",
        "                 **kwargs):\n",
        "        super(EpochModelCheckpoint, self).__init__(filepath, monitor, verbose, save_best_only, save_weights_only,\n",
        "                                                   mode, \"epoch\", options)\n",
        "        self.epochs_since_last_save = 0\n",
        "        self.frequency = frequency\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.epochs_since_last_save += 1\n",
        "        # pylint: disable=protected-access\n",
        "        if self.epochs_since_last_save % self.frequency == 0:\n",
        "            self._save_model(epoch=epoch, batch=None, logs=logs)\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        pass"
      ],
      "metadata": {
        "id": "cfBZbLrNxgxy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# due to the size of the tensors, the call below fails (out of memory) when it tries to run the callback EvaluateEpochEnd.\n",
        "#history = model.fit(x=X_train, y=y_train, epochs=num_epochs, validation_data=validation_data, callbacks=[cp, reduce_lr, EvaluateEpochEnd((X_test, y_test))])\n",
        "\n",
        "# ALTERNATIVE: therefore I used another callback that saves the model every 10 epochs. I calculate the test loss and accuracy on these checkpoints after the run is finished.\n",
        "history = model.fit(x=X_train, y=y_train, epochs=num_epochs, validation_data=validation_data, callbacks=[cp, reduce_lr, EpochModelCheckpoint(model_file_prefix + '_epoch_{epoch:03d}.h5', frequency=10)])\n",
        "\n",
        "\n",
        "#short:\n",
        "#history = model.fit(x=X_train, y=y_train, epochs=num_epochs, validation_data=validation_data, callbacks=[cp, es])\n",
        "\n",
        "\n",
        "# print all the run history, that can be used later on for the graphs\n",
        "print(\"RUN HISTORY:\\n\")\n",
        "print(history.history.keys())\n",
        "print(history.history.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN4LKzJR6rqR",
        "outputId": "dfa7e6b0-1982-4ace-fbb4-d8af09ea52bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 4.0027 - accuracy: 0.1558\n",
            "Epoch 1: val_accuracy improved from -inf to 0.38984, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 44s 297ms/step - loss: 4.0027 - accuracy: 0.1558 - val_loss: 2.7548 - val_accuracy: 0.3898 - lr: 1.0000e-04\n",
            "Epoch 2/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 2.3568 - accuracy: 0.4985\n",
            "Epoch 2: val_accuracy improved from 0.38984 to 0.63556, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 255ms/step - loss: 2.3568 - accuracy: 0.4985 - val_loss: 1.8699 - val_accuracy: 0.6356 - lr: 1.0000e-04\n",
            "Epoch 3/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.6449 - accuracy: 0.6629\n",
            "Epoch 3: val_accuracy improved from 0.63556 to 0.72545, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 254ms/step - loss: 1.6449 - accuracy: 0.6629 - val_loss: 1.4463 - val_accuracy: 0.7255 - lr: 1.0000e-04\n",
            "Epoch 4/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.2311 - accuracy: 0.7650\n",
            "Epoch 4: val_accuracy improved from 0.72545 to 0.78065, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 254ms/step - loss: 1.2311 - accuracy: 0.7650 - val_loss: 1.1800 - val_accuracy: 0.7807 - lr: 1.0000e-04\n",
            "Epoch 5/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9654 - accuracy: 0.8244\n",
            "Epoch 5: val_accuracy improved from 0.78065 to 0.82022, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.9654 - accuracy: 0.8244 - val_loss: 0.9995 - val_accuracy: 0.8202 - lr: 1.0000e-04\n",
            "Epoch 6/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7791 - accuracy: 0.8647\n",
            "Epoch 6: val_accuracy improved from 0.82022 to 0.84612, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 255ms/step - loss: 0.7791 - accuracy: 0.8647 - val_loss: 0.8687 - val_accuracy: 0.8461 - lr: 1.0000e-04\n",
            "Epoch 7/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6340 - accuracy: 0.8964\n",
            "Epoch 7: val_accuracy improved from 0.84612 to 0.86468, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 255ms/step - loss: 0.6340 - accuracy: 0.8964 - val_loss: 0.7687 - val_accuracy: 0.8647 - lr: 1.0000e-04\n",
            "Epoch 8/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.5375 - accuracy: 0.9174\n",
            "Epoch 8: val_accuracy improved from 0.86468 to 0.87689, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 256ms/step - loss: 0.5375 - accuracy: 0.9174 - val_loss: 0.6898 - val_accuracy: 0.8769 - lr: 1.0000e-04\n",
            "Epoch 9/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.9294\n",
            "Epoch 9: val_accuracy improved from 0.87689 to 0.88471, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 254ms/step - loss: 0.4690 - accuracy: 0.9294 - val_loss: 0.6284 - val_accuracy: 0.8847 - lr: 1.0000e-04\n",
            "Epoch 10/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.4032 - accuracy: 0.9394\n",
            "Epoch 10: val_accuracy improved from 0.88471 to 0.89546, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 257ms/step - loss: 0.4032 - accuracy: 0.9394 - val_loss: 0.5794 - val_accuracy: 0.8955 - lr: 1.0000e-04\n",
            "Epoch 11/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.9541\n",
            "Epoch 11: val_accuracy improved from 0.89546 to 0.89741, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 254ms/step - loss: 0.3417 - accuracy: 0.9541 - val_loss: 0.5393 - val_accuracy: 0.8974 - lr: 1.0000e-04\n",
            "Epoch 12/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.2939 - accuracy: 0.9641\n",
            "Epoch 12: val_accuracy improved from 0.89741 to 0.90278, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 254ms/step - loss: 0.2939 - accuracy: 0.9641 - val_loss: 0.5036 - val_accuracy: 0.9028 - lr: 1.0000e-04\n",
            "Epoch 13/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.9692\n",
            "Epoch 13: val_accuracy improved from 0.90278 to 0.90816, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 254ms/step - loss: 0.2533 - accuracy: 0.9692 - val_loss: 0.4757 - val_accuracy: 0.9082 - lr: 1.0000e-04\n",
            "Epoch 14/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.2273 - accuracy: 0.9734\n",
            "Epoch 14: val_accuracy improved from 0.90816 to 0.91109, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.2273 - accuracy: 0.9734 - val_loss: 0.4493 - val_accuracy: 0.9111 - lr: 1.0000e-04\n",
            "Epoch 15/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.9795\n",
            "Epoch 15: val_accuracy improved from 0.91109 to 0.91451, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.2063 - accuracy: 0.9795 - val_loss: 0.4297 - val_accuracy: 0.9145 - lr: 1.0000e-04\n",
            "Epoch 16/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.1827 - accuracy: 0.9856\n",
            "Epoch 16: val_accuracy did not improve from 0.91451\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.1827 - accuracy: 0.9856 - val_loss: 0.4140 - val_accuracy: 0.9135 - lr: 1.0000e-04\n",
            "Epoch 17/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.1591 - accuracy: 0.9853\n",
            "Epoch 17: val_accuracy improved from 0.91451 to 0.91597, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 254ms/step - loss: 0.1591 - accuracy: 0.9853 - val_loss: 0.3965 - val_accuracy: 0.9160 - lr: 1.0000e-04\n",
            "Epoch 18/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.1457 - accuracy: 0.9856\n",
            "Epoch 18: val_accuracy improved from 0.91597 to 0.91988, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.1457 - accuracy: 0.9856 - val_loss: 0.3828 - val_accuracy: 0.9199 - lr: 1.0000e-04\n",
            "Epoch 19/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9907\n",
            "Epoch 19: val_accuracy improved from 0.91988 to 0.92281, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 254ms/step - loss: 0.1292 - accuracy: 0.9907 - val_loss: 0.3694 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
            "Epoch 20/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.9893\n",
            "Epoch 20: val_accuracy did not improve from 0.92281\n",
            "128/128 [==============================] - 32s 248ms/step - loss: 0.1182 - accuracy: 0.9893 - val_loss: 0.3590 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
            "Epoch 21/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9922\n",
            "Epoch 21: val_accuracy did not improve from 0.92281\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.1050 - accuracy: 0.9922 - val_loss: 0.3494 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
            "Epoch 22/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9941\n",
            "Epoch 22: val_accuracy did not improve from 0.92281\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0921 - accuracy: 0.9941 - val_loss: 0.3410 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
            "Epoch 23/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9954\n",
            "Epoch 23: val_accuracy did not improve from 0.92281\n",
            "128/128 [==============================] - 32s 248ms/step - loss: 0.0873 - accuracy: 0.9954 - val_loss: 0.3325 - val_accuracy: 0.9223 - lr: 1.0000e-04\n",
            "Epoch 24/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9939\n",
            "Epoch 24: val_accuracy improved from 0.92281 to 0.92477, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.0792 - accuracy: 0.9939 - val_loss: 0.3280 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
            "Epoch 25/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9941\n",
            "Epoch 25: val_accuracy did not improve from 0.92477\n",
            "128/128 [==============================] - 32s 247ms/step - loss: 0.0757 - accuracy: 0.9941 - val_loss: 0.3213 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
            "Epoch 26/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9980\n",
            "Epoch 26: val_accuracy did not improve from 0.92477\n",
            "128/128 [==============================] - 31s 247ms/step - loss: 0.0620 - accuracy: 0.9980 - val_loss: 0.3159 - val_accuracy: 0.9243 - lr: 1.0000e-04\n",
            "Epoch 27/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9976\n",
            "Epoch 27: val_accuracy improved from 0.92477 to 0.92819, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 254ms/step - loss: 0.0586 - accuracy: 0.9976 - val_loss: 0.3098 - val_accuracy: 0.9282 - lr: 1.0000e-04\n",
            "Epoch 28/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9985\n",
            "Epoch 28: val_accuracy did not improve from 0.92819\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0520 - accuracy: 0.9985 - val_loss: 0.3026 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
            "Epoch 29/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9978\n",
            "Epoch 29: val_accuracy did not improve from 0.92819\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0495 - accuracy: 0.9978 - val_loss: 0.2972 - val_accuracy: 0.9272 - lr: 1.0000e-04\n",
            "Epoch 30/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9983\n",
            "Epoch 30: val_accuracy improved from 0.92819 to 0.92916, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 257ms/step - loss: 0.0458 - accuracy: 0.9983 - val_loss: 0.2935 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
            "Epoch 31/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9988\n",
            "Epoch 31: val_accuracy improved from 0.92916 to 0.93014, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.0415 - accuracy: 0.9988 - val_loss: 0.2896 - val_accuracy: 0.9301 - lr: 1.0000e-04\n",
            "Epoch 32/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9993\n",
            "Epoch 32: val_accuracy did not improve from 0.93014\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0392 - accuracy: 0.9993 - val_loss: 0.2866 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
            "Epoch 33/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9983\n",
            "Epoch 33: val_accuracy did not improve from 0.93014\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0379 - accuracy: 0.9983 - val_loss: 0.2853 - val_accuracy: 0.9267 - lr: 1.0000e-04\n",
            "Epoch 34/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9995\n",
            "Epoch 34: val_accuracy did not improve from 0.93014\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0306 - accuracy: 0.9995 - val_loss: 0.2788 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
            "Epoch 35/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9998\n",
            "Epoch 35: val_accuracy did not improve from 0.93014\n",
            "128/128 [==============================] - 31s 247ms/step - loss: 0.0304 - accuracy: 0.9998 - val_loss: 0.2773 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
            "Epoch 36/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9998\n",
            "Epoch 36: val_accuracy did not improve from 0.93014\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0261 - accuracy: 0.9998 - val_loss: 0.2727 - val_accuracy: 0.9272 - lr: 1.0000e-04\n",
            "Epoch 37/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9990\n",
            "Epoch 37: val_accuracy did not improve from 0.93014\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0254 - accuracy: 0.9990 - val_loss: 0.2729 - val_accuracy: 0.9267 - lr: 1.0000e-04\n",
            "Epoch 38/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9998\n",
            "Epoch 38: val_accuracy did not improve from 0.93014\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0235 - accuracy: 0.9998 - val_loss: 0.2732 - val_accuracy: 0.9267 - lr: 1.0000e-04\n",
            "Epoch 39/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9995\n",
            "Epoch 39: val_accuracy improved from 0.93014 to 0.93112, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.0241 - accuracy: 0.9995 - val_loss: 0.2712 - val_accuracy: 0.9311 - lr: 1.0000e-04\n",
            "Epoch 40/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9995\n",
            "Epoch 40: val_accuracy did not improve from 0.93112\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 0.0205 - accuracy: 0.9995 - val_loss: 0.2687 - val_accuracy: 0.9301 - lr: 1.0000e-04\n",
            "Epoch 41/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9995\n",
            "Epoch 41: val_accuracy improved from 0.93112 to 0.93258, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 255ms/step - loss: 0.0210 - accuracy: 0.9995 - val_loss: 0.2652 - val_accuracy: 0.9326 - lr: 1.0000e-04\n",
            "Epoch 42/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9995\n",
            "Epoch 42: val_accuracy did not improve from 0.93258\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0193 - accuracy: 0.9995 - val_loss: 0.2629 - val_accuracy: 0.9282 - lr: 1.0000e-04\n",
            "Epoch 43/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9998\n",
            "Epoch 43: val_accuracy did not improve from 0.93258\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0159 - accuracy: 0.9998 - val_loss: 0.2587 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
            "Epoch 44/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9998\n",
            "Epoch 44: val_accuracy did not improve from 0.93258\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0154 - accuracy: 0.9998 - val_loss: 0.2583 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
            "Epoch 45/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 45: val_accuracy did not improve from 0.93258\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.2563 - val_accuracy: 0.9326 - lr: 1.0000e-04\n",
            "Epoch 46/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9998\n",
            "Epoch 46: val_accuracy did not improve from 0.93258\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0137 - accuracy: 0.9998 - val_loss: 0.2548 - val_accuracy: 0.9316 - lr: 1.0000e-04\n",
            "Epoch 47/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 47: val_accuracy did not improve from 0.93258\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.2561 - val_accuracy: 0.9297 - lr: 1.0000e-04\n",
            "Epoch 48/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9995\n",
            "Epoch 48: val_accuracy improved from 0.93258 to 0.93405, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 32s 254ms/step - loss: 0.0130 - accuracy: 0.9995 - val_loss: 0.2515 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
            "Epoch 49/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9995\n",
            "Epoch 49: val_accuracy improved from 0.93405 to 0.93454, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.0126 - accuracy: 0.9995 - val_loss: 0.2525 - val_accuracy: 0.9345 - lr: 1.0000e-04\n",
            "Epoch 50/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9998\n",
            "Epoch 50: val_accuracy did not improve from 0.93454\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 0.0115 - accuracy: 0.9998 - val_loss: 0.2514 - val_accuracy: 0.9331 - lr: 1.0000e-04\n",
            "Epoch 51/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9998\n",
            "Epoch 51: val_accuracy did not improve from 0.93454\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0110 - accuracy: 0.9998 - val_loss: 0.2487 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
            "Epoch 52/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9995\n",
            "Epoch 52: val_accuracy improved from 0.93454 to 0.93747, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.0098 - accuracy: 0.9995 - val_loss: 0.2453 - val_accuracy: 0.9375 - lr: 1.0000e-04\n",
            "Epoch 53/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.9995\n",
            "Epoch 53: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0097 - accuracy: 0.9995 - val_loss: 0.2461 - val_accuracy: 0.9345 - lr: 1.0000e-04\n",
            "Epoch 54/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.9998\n",
            "Epoch 54: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0093 - accuracy: 0.9998 - val_loss: 0.2441 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
            "Epoch 55/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9998\n",
            "Epoch 55: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0085 - accuracy: 0.9998 - val_loss: 0.2451 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
            "Epoch 56/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9995\n",
            "Epoch 56: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0088 - accuracy: 0.9995 - val_loss: 0.2411 - val_accuracy: 0.9360 - lr: 1.0000e-04\n",
            "Epoch 57/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9998\n",
            "Epoch 57: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0065 - accuracy: 0.9998 - val_loss: 0.2511 - val_accuracy: 0.9306 - lr: 1.0000e-04\n",
            "Epoch 58/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9998\n",
            "Epoch 58: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0074 - accuracy: 0.9998 - val_loss: 0.2415 - val_accuracy: 0.9345 - lr: 1.0000e-04\n",
            "Epoch 59/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9998\n",
            "Epoch 59: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0068 - accuracy: 0.9998 - val_loss: 0.2395 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
            "Epoch 60/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9995\n",
            "Epoch 60: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 32s 248ms/step - loss: 0.0064 - accuracy: 0.9995 - val_loss: 0.2391 - val_accuracy: 0.9375 - lr: 1.0000e-04\n",
            "Epoch 61/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 61: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.2402 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
            "Epoch 62/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9993\n",
            "Epoch 62: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0060 - accuracy: 0.9993 - val_loss: 0.2396 - val_accuracy: 0.9375 - lr: 1.0000e-04\n",
            "Epoch 63/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9995\n",
            "Epoch 63: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0057 - accuracy: 0.9995 - val_loss: 0.2401 - val_accuracy: 0.9365 - lr: 1.0000e-04\n",
            "Epoch 64/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 64: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 247ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.2390 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
            "Epoch 65/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 65: val_accuracy did not improve from 0.93747\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.2410 - val_accuracy: 0.9336 - lr: 1.0000e-04\n",
            "Epoch 66/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 66: val_accuracy improved from 0.93747 to 0.93845, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
            "Epoch 67/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9993\n",
            "Epoch 67: val_accuracy did not improve from 0.93845\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0058 - accuracy: 0.9993 - val_loss: 0.2398 - val_accuracy: 0.9375 - lr: 1.0000e-04\n",
            "Epoch 68/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9998\n",
            "Epoch 68: val_accuracy did not improve from 0.93845\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0040 - accuracy: 0.9998 - val_loss: 0.2395 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
            "Epoch 69/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 69: val_accuracy improved from 0.93845 to 0.93942, saving model to /content/drive/MyDrive/Data Science/BGU/Machine Learning/Assignments/3/model_files/1007/0_best_model.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2350 - val_accuracy: 0.9394 - lr: 1.0000e-04\n",
            "Epoch 70/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9998\n",
            "Epoch 70: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 0.0033 - accuracy: 0.9998 - val_loss: 0.2338 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
            "Epoch 71/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9998\n",
            "Epoch 71: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0048 - accuracy: 0.9998 - val_loss: 0.2366 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
            "Epoch 72/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9995\n",
            "Epoch 72: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.2345 - val_accuracy: 0.9355 - lr: 1.0000e-04\n",
            "Epoch 73/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 73: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2336 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
            "Epoch 74/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 74: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2375 - val_accuracy: 0.9355 - lr: 1.0000e-04\n",
            "Epoch 75/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 0.9998\n",
            "Epoch 75: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0031 - accuracy: 0.9998 - val_loss: 0.2335 - val_accuracy: 0.9389 - lr: 1.0000e-04\n",
            "Epoch 76/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9998\n",
            "Epoch 76: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0038 - accuracy: 0.9998 - val_loss: 0.2370 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
            "Epoch 77/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 77: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2351 - val_accuracy: 0.9389 - lr: 1.0000e-04\n",
            "Epoch 78/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 78: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2376 - val_accuracy: 0.9365 - lr: 1.0000e-04\n",
            "Epoch 79/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 79: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.2396 - val_accuracy: 0.9331 - lr: 1.0000e-04\n",
            "Epoch 80/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 80: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 32s 248ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2335 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
            "Epoch 81/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 81: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2379 - val_accuracy: 0.9365 - lr: 1.0000e-04\n",
            "Epoch 82/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9998\n",
            "Epoch 82: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.2399 - val_accuracy: 0.9345 - lr: 1.0000e-04\n",
            "Epoch 83/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9998\n",
            "Epoch 83: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0032 - accuracy: 0.9998 - val_loss: 0.2370 - val_accuracy: 0.9375 - lr: 1.0000e-04\n",
            "Epoch 84/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9998\n",
            "Epoch 84: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.2432 - val_accuracy: 0.9326 - lr: 1.0000e-04\n",
            "Epoch 85/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9995\n",
            "Epoch 85: val_accuracy did not improve from 0.93942\n",
            "\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.2335 - val_accuracy: 0.9375 - lr: 1.0000e-04\n",
            "Epoch 86/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9998\n",
            "Epoch 86: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 0.2336 - val_accuracy: 0.9375 - lr: 1.0000e-05\n",
            "Epoch 87/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9998\n",
            "Epoch 87: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 247ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.2343 - val_accuracy: 0.9380 - lr: 1.0000e-05\n",
            "Epoch 88/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9998\n",
            "Epoch 88: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.2346 - val_accuracy: 0.9380 - lr: 1.0000e-05\n",
            "Epoch 89/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 89: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2343 - val_accuracy: 0.9384 - lr: 1.0000e-05\n",
            "Epoch 90/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9998\n",
            "Epoch 90: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 32s 248ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.2340 - val_accuracy: 0.9389 - lr: 1.0000e-05\n",
            "Epoch 91/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
            "Epoch 91: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2333 - val_accuracy: 0.9394 - lr: 1.0000e-05\n",
            "Epoch 92/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9998\n",
            "Epoch 92: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.2331 - val_accuracy: 0.9389 - lr: 1.0000e-05\n",
            "Epoch 93/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9998\n",
            "Epoch 93: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.2330 - val_accuracy: 0.9384 - lr: 1.0000e-05\n",
            "Epoch 94/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 94: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2333 - val_accuracy: 0.9380 - lr: 1.0000e-05\n",
            "Epoch 95/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 95: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2326 - val_accuracy: 0.9380 - lr: 1.0000e-05\n",
            "Epoch 96/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 96: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2324 - val_accuracy: 0.9380 - lr: 1.0000e-05\n",
            "Epoch 97/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 97: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2330 - val_accuracy: 0.9384 - lr: 1.0000e-05\n",
            "Epoch 98/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 98: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2329 - val_accuracy: 0.9380 - lr: 1.0000e-05\n",
            "Epoch 99/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 99: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 247ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2323 - val_accuracy: 0.9380 - lr: 1.0000e-05\n",
            "Epoch 100/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 100: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 32s 248ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2323 - val_accuracy: 0.9380 - lr: 1.0000e-05\n",
            "Epoch 101/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 101: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2307 - val_accuracy: 0.9380 - lr: 1.0000e-05\n",
            "Epoch 102/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 102: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2310 - val_accuracy: 0.9375 - lr: 1.0000e-05\n",
            "Epoch 103/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 103: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2312 - val_accuracy: 0.9375 - lr: 1.0000e-05\n",
            "Epoch 104/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 104: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2319 - val_accuracy: 0.9375 - lr: 1.0000e-05\n",
            "Epoch 105/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 105: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2310 - val_accuracy: 0.9389 - lr: 1.0000e-05\n",
            "Epoch 106/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 106: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2304 - val_accuracy: 0.9389 - lr: 1.0000e-05\n",
            "Epoch 107/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 107: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2305 - val_accuracy: 0.9389 - lr: 1.0000e-05\n",
            "Epoch 108/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 108: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2303 - val_accuracy: 0.9384 - lr: 1.0000e-05\n",
            "Epoch 109/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
            "Epoch 109: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.2308 - val_accuracy: 0.9389 - lr: 1.0000e-05\n",
            "Epoch 110/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 9.9175e-04 - accuracy: 1.0000\n",
            "Epoch 110: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 9.9175e-04 - accuracy: 1.0000 - val_loss: 0.2313 - val_accuracy: 0.9375 - lr: 1.0000e-05\n",
            "Epoch 111/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 111: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2313 - val_accuracy: 0.9375 - lr: 1.0000e-05\n",
            "Epoch 112/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 112: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2322 - val_accuracy: 0.9380 - lr: 1.0000e-05\n",
            "Epoch 113/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9995\n",
            "Epoch 113: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.2312 - val_accuracy: 0.9384 - lr: 1.0000e-05\n",
            "Epoch 114/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 114: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2309 - val_accuracy: 0.9394 - lr: 1.0000e-05\n",
            "Epoch 115/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 115: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2304 - val_accuracy: 0.9394 - lr: 1.0000e-05\n",
            "Epoch 116/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 116: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2313 - val_accuracy: 0.9375 - lr: 1.0000e-05\n",
            "Epoch 117/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 117: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2328 - val_accuracy: 0.9375 - lr: 1.0000e-05\n",
            "Epoch 118/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 9.3622e-04 - accuracy: 1.0000\n",
            "Epoch 118: val_accuracy did not improve from 0.93942\n",
            "\n",
            "Epoch 118: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 9.3622e-04 - accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9375 - lr: 1.0000e-05\n",
            "Epoch 119/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 119: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 31s 246ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2325 - val_accuracy: 0.9375 - lr: 1.0000e-06\n",
            "Epoch 120/120\n",
            "128/128 [==============================] - ETA: 0s - loss: 9.7767e-04 - accuracy: 1.0000\n",
            "Epoch 120: val_accuracy did not improve from 0.93942\n",
            "128/128 [==============================] - 32s 248ms/step - loss: 9.7767e-04 - accuracy: 1.0000 - val_loss: 0.2321 - val_accuracy: 0.9375 - lr: 1.0000e-06\n",
            "RUN HISTORY:\n",
            "\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n",
            "dict_values([[4.002749919891357, 2.3567914962768555, 1.6448819637298584, 1.231070876121521, 0.9653895497322083, 0.7791293263435364, 0.6339713931083679, 0.5375135540962219, 0.46897202730178833, 0.403186559677124, 0.34170758724212646, 0.29393014311790466, 0.2533170282840729, 0.2273043692111969, 0.20629949867725372, 0.18271291255950928, 0.15906845033168793, 0.14571258425712585, 0.12917377054691315, 0.11821120977401733, 0.1050148457288742, 0.0920613706111908, 0.08734448999166489, 0.07923590391874313, 0.07566289603710175, 0.06200714409351349, 0.05856749787926674, 0.05203799530863762, 0.049479540437459946, 0.0458468459546566, 0.04146125540137291, 0.03924906626343727, 0.03790505602955818, 0.030630428344011307, 0.03036915883421898, 0.026098867878317833, 0.025366410613059998, 0.023460805416107178, 0.02405346930027008, 0.020490940660238266, 0.02095026895403862, 0.019328055903315544, 0.015870226547122, 0.015352610498666763, 0.014899814501404762, 0.0137234628200531, 0.012423000298440456, 0.013036378659307957, 0.01264007668942213, 0.011484858579933643, 0.011003796942532063, 0.009768437594175339, 0.009735455736517906, 0.009304321371018887, 0.008471020497381687, 0.008816372603178024, 0.006525223609060049, 0.007433151360601187, 0.0068046581000089645, 0.006447437684983015, 0.00523481285199523, 0.006022890098392963, 0.00569327874109149, 0.004372762516140938, 0.004181177821010351, 0.0042774248868227005, 0.005840040743350983, 0.0040138703770935535, 0.0035314345732331276, 0.0033309480641037226, 0.004774787463247776, 0.004079216159880161, 0.0030848130118101835, 0.002783432835713029, 0.0031178086064755917, 0.003769750939682126, 0.0023722269106656313, 0.0022239128593355417, 0.002550967037677765, 0.0021760929375886917, 0.002142741810530424, 0.002131086541339755, 0.003245482686907053, 0.0023041414096951485, 0.003393626306205988, 0.0035453862510621548, 0.0023017232306301594, 0.00195799651555717, 0.0014089823234826326, 0.0019350916845723987, 0.001624346012249589, 0.002152335597202182, 0.002199396025389433, 0.0014835258480161428, 0.0015447074547410011, 0.0013765323674306273, 0.00131128984503448, 0.0015799292596057057, 0.0013862964697182178, 0.0013236243976280093, 0.001385875977575779, 0.0013095824979245663, 0.0012868550838902593, 0.0015070143854245543, 0.0010937496554106474, 0.001181165687739849, 0.001109453965909779, 0.0014503814745694399, 0.0020092197228223085, 0.0009917489951476455, 0.0010510600404813886, 0.0012295665219426155, 0.0014289868995547295, 0.0015422458527609706, 0.001099735265597701, 0.0010570777812972665, 0.0010819551534950733, 0.0009362242417410016, 0.0010821650503203273, 0.0009776722872629762], [0.1558378040790558, 0.4985344409942627, 0.6629213690757751, 0.7650219798088074, 0.8243771195411682, 0.8646799921989441, 0.8964338302612305, 0.9174401760101318, 0.9294089078903198, 0.9394235610961914, 0.9540791511535645, 0.964093804359436, 0.9692232608795166, 0.973375678062439, 0.9794821739196777, 0.9855886697769165, 0.985344409942627, 0.9855886697769165, 0.9907181262969971, 0.9892525672912598, 0.9921836853027344, 0.9941377639770508, 0.9953590631484985, 0.9938935041427612, 0.9941377639770508, 0.9980459213256836, 0.9975574016571045, 0.9985344409942627, 0.997801661491394, 0.9982901811599731, 0.9987787008285522, 0.9992672204971313, 0.9982901811599731, 0.9995114803314209, 0.9997557401657104, 0.9997557401657104, 0.9990229606628418, 0.9997557401657104, 0.9995114803314209, 0.9995114803314209, 0.9995114803314209, 0.9995114803314209, 0.9997557401657104, 0.9997557401657104, 1.0, 0.9997557401657104, 1.0, 0.9995114803314209, 0.9995114803314209, 0.9997557401657104, 0.9997557401657104, 0.9995114803314209, 0.9995114803314209, 0.9997557401657104, 0.9997557401657104, 0.9995114803314209, 0.9997557401657104, 0.9997557401657104, 0.9997557401657104, 0.9995114803314209, 1.0, 0.9992672204971313, 0.9995114803314209, 1.0, 1.0, 1.0, 0.9992672204971313, 0.9997557401657104, 1.0, 0.9997557401657104, 0.9997557401657104, 0.9995114803314209, 1.0, 1.0, 0.9997557401657104, 0.9997557401657104, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9997557401657104, 0.9997557401657104, 0.9997557401657104, 0.9995114803314209, 0.9997557401657104, 0.9997557401657104, 0.9997557401657104, 1.0, 0.9997557401657104, 0.9997557401657104, 0.9997557401657104, 0.9997557401657104, 1.0, 0.9997557401657104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9997557401657104, 1.0, 1.0, 1.0, 0.9997557401657104, 0.9995114803314209, 1.0, 1.0, 1.0, 0.9995114803314209, 0.9997557401657104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [2.7548255920410156, 1.8698618412017822, 1.4462674856185913, 1.1799722909927368, 0.9994950890541077, 0.8687167167663574, 0.7687432765960693, 0.689835250377655, 0.628446102142334, 0.5793552398681641, 0.5393384099006653, 0.5035934448242188, 0.4757463335990906, 0.4493301212787628, 0.4296703636646271, 0.41404998302459717, 0.3964708149433136, 0.3827672004699707, 0.3693639934062958, 0.35897570848464966, 0.3493894040584564, 0.340977281332016, 0.33254292607307434, 0.3279922306537628, 0.3213379979133606, 0.31591418385505676, 0.3097774088382721, 0.3025968670845032, 0.2971835136413574, 0.2934732437133789, 0.2896493375301361, 0.28660088777542114, 0.28530776500701904, 0.27883589267730713, 0.2773316204547882, 0.2727363407611847, 0.27285245060920715, 0.2731570899486542, 0.2711571455001831, 0.26871734857559204, 0.2651574909687042, 0.26285603642463684, 0.25871264934539795, 0.2583317458629608, 0.25629863142967224, 0.2547662854194641, 0.2560961842536926, 0.2515122592449188, 0.25254344940185547, 0.2513563334941864, 0.24871782958507538, 0.24527974426746368, 0.2461041361093521, 0.2440771758556366, 0.24508628249168396, 0.24106459319591522, 0.2511272132396698, 0.24147041141986847, 0.23949649930000305, 0.23913335800170898, 0.24024660885334015, 0.23956727981567383, 0.24005019664764404, 0.23898287117481232, 0.24098865687847137, 0.23812025785446167, 0.2398337721824646, 0.23951618373394012, 0.23497937619686127, 0.2338082641363144, 0.23664572834968567, 0.23452086746692657, 0.23361973464488983, 0.2374606877565384, 0.23350612819194794, 0.23698236048221588, 0.23508551716804504, 0.237605020403862, 0.23958757519721985, 0.23345966637134552, 0.23789997398853302, 0.2398747056722641, 0.2370360791683197, 0.24324257671833038, 0.23349888622760773, 0.233613982796669, 0.23425140976905823, 0.2346397340297699, 0.23431159555912018, 0.23397132754325867, 0.23326192796230316, 0.23312795162200928, 0.23299455642700195, 0.23327796161174774, 0.23264287412166595, 0.2324303835630417, 0.2330002337694168, 0.2329348474740982, 0.23227594792842865, 0.23226334154605865, 0.23069600760936737, 0.23096740245819092, 0.2311747819185257, 0.23185789585113525, 0.23102575540542603, 0.23040559887886047, 0.23052316904067993, 0.23026970028877258, 0.23078790307044983, 0.23133768141269684, 0.2312517613172531, 0.23217551410198212, 0.23121441900730133, 0.2309279590845108, 0.23036231100559235, 0.23132513463497162, 0.2328229397535324, 0.23261822760105133, 0.23247498273849487, 0.23206846415996552], [0.3898387849330902, 0.6355642676353455, 0.7254518866539001, 0.7806546092033386, 0.8202247023582458, 0.8461162447929382, 0.8646799921989441, 0.8768930435180664, 0.884709358215332, 0.8954567909240723, 0.8974108695983887, 0.9027845859527588, 0.9081583023071289, 0.9110894203186035, 0.9145090579986572, 0.913532018661499, 0.9159746170043945, 0.9198827743530273, 0.922813892364502, 0.9213483333587646, 0.9213483333587646, 0.9213483333587646, 0.9223253726959229, 0.9247679710388184, 0.9247679710388184, 0.9242794513702393, 0.9281876087188721, 0.9262335300445557, 0.9272105693817139, 0.9291646480560303, 0.9301416873931885, 0.9291646480560303, 0.9267220497131348, 0.9291646480560303, 0.9262335300445557, 0.9272105693817139, 0.9267220497131348, 0.9267220497131348, 0.9311187267303467, 0.9301416873931885, 0.932584285736084, 0.9281876087188721, 0.9291646480560303, 0.9291646480560303, 0.932584285736084, 0.9316072463989258, 0.9296531677246094, 0.9340498447418213, 0.9345383644104004, 0.9330728054046631, 0.9340498447418213, 0.937469482421875, 0.9345383644104004, 0.9350268840789795, 0.9340498447418213, 0.9360039234161377, 0.9306302070617676, 0.9345383644104004, 0.9369809627532959, 0.937469482421875, 0.9369809627532959, 0.937469482421875, 0.9364924430847168, 0.9369809627532959, 0.9335613250732422, 0.9384465217590332, 0.937469482421875, 0.9369809627532959, 0.9394235610961914, 0.9379580020904541, 0.9384465217590332, 0.9355154037475586, 0.9384465217590332, 0.9355154037475586, 0.9389350414276123, 0.9379580020904541, 0.9389350414276123, 0.9364924430847168, 0.9330728054046631, 0.9369809627532959, 0.9364924430847168, 0.9345383644104004, 0.937469482421875, 0.932584285736084, 0.937469482421875, 0.937469482421875, 0.9379580020904541, 0.9379580020904541, 0.9384465217590332, 0.9389350414276123, 0.9394235610961914, 0.9389350414276123, 0.9384465217590332, 0.9379580020904541, 0.9379580020904541, 0.9379580020904541, 0.9384465217590332, 0.9379580020904541, 0.9379580020904541, 0.9379580020904541, 0.9379580020904541, 0.937469482421875, 0.937469482421875, 0.937469482421875, 0.9389350414276123, 0.9389350414276123, 0.9389350414276123, 0.9384465217590332, 0.9389350414276123, 0.937469482421875, 0.937469482421875, 0.9379580020904541, 0.9384465217590332, 0.9394235610961914, 0.9394235610961914, 0.937469482421875, 0.937469482421875, 0.937469482421875, 0.937469482421875, 0.937469482421875], [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-06, 1e-06]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train"
      ],
      "metadata": {
        "id": "AdCK9Hdiy-Pa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as mentioned in the ALTERNATIVE above, after the run the saved models are loaded to predict the test accuracy and loss every 10th epoch \n",
        "for i in range(10, num_epochs+1, 10):\n",
        "  final_model = load_model(f'{model_file_prefix}_epoch_{i:03d}.h5')\n",
        "  scores = final_model.evaluate(X_test,y_test)\n",
        "  history_test['test_loss'].append(scores[0])\n",
        "  history_test['test_accuracy'].append(scores[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gmgnf18Jag_",
        "outputId": "1516574d-9ee8-4b96-da5a-1c118a82dac6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64/64 [==============================] - 11s 164ms/step - loss: 0.5615 - accuracy: 0.8877\n",
            "64/64 [==============================] - 13s 206ms/step - loss: 0.3552 - accuracy: 0.9199\n",
            "64/64 [==============================] - 13s 207ms/step - loss: 0.2958 - accuracy: 0.9258\n",
            "64/64 [==============================] - 11s 163ms/step - loss: 0.2749 - accuracy: 0.9277\n",
            "64/64 [==============================] - 11s 163ms/step - loss: 0.2602 - accuracy: 0.9292\n",
            "64/64 [==============================] - 11s 164ms/step - loss: 0.2525 - accuracy: 0.9287\n",
            "64/64 [==============================] - 11s 163ms/step - loss: 0.2494 - accuracy: 0.9312\n",
            "64/64 [==============================] - 11s 164ms/step - loss: 0.2440 - accuracy: 0.9307\n",
            "64/64 [==============================] - 11s 164ms/step - loss: 0.2453 - accuracy: 0.9331\n",
            "64/64 [==============================] - 11s 163ms/step - loss: 0.2452 - accuracy: 0.9321\n",
            "64/64 [==============================] - 11s 163ms/step - loss: 0.2443 - accuracy: 0.9307\n",
            "64/64 [==============================] - 13s 207ms/step - loss: 0.2439 - accuracy: 0.9326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data = {}\n",
        "\n",
        "plot_data_keys = list(history.history.keys())\n",
        "#plot_data_keys = ['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr']\n",
        "plot_data_values = list(history.history.values())\n",
        "#plot_data_values = [lss0,acc0,lss,acc,[]]\n",
        "\n",
        "for i in range(len(plot_data_keys)):\n",
        "  plot_data[plot_data_keys[i]] = plot_data_values[i]\n",
        "  "
      ],
      "metadata": {
        "id": "KyejtHjwMGK5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best model predictions\n",
        "final_model = load_model(model_file_prefix + '_best_model.h5')\n",
        "scores = final_model.evaluate(X_test,y_test)\n",
        "print(scores)\n",
        "\n",
        "best_epic_number = 69\n",
        "\n",
        "# will be used in the plot"
      ],
      "metadata": {
        "id": "D0ng8ImXZsCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04491297-7702-44ef-a192-388421ba7bd9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64/64 [==============================] - 13s 206ms/step - loss: 0.2490 - accuracy: 0.9312\n",
            "[0.24896840751171112, 0.93115234375]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EHYcZeu8j-mM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "outputId": "8e26b40d-7653-4510-873a-02a391253217"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "num_of_x_values = len(plot_data_values[0])\n",
        "\n",
        "# summarize history for accuracy\n",
        "fig = plt.figure(figsize = (10, 7))\n",
        "plt.plot(np.arange(1,num_of_x_values+1), plot_data['accuracy'])\n",
        "plt.plot(np.arange(1,num_of_x_values+1), plot_data['val_accuracy'])\n",
        "plt.scatter(np.arange(10,num_of_x_values+1, 10), history_test['test_accuracy'], s=50, color='green')\n",
        "plt.scatter([best_epic_number], scores[1], s=50, color='green', marker='x')\n",
        "\n",
        "\n",
        "plt.title('Accuracy per Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.xticks(np.arange(0,num_of_x_values+1,10))\n",
        "\n",
        "plt.legend(['train', 'validation', 'test', 'best'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "fig = plt.figure(figsize = (10, 7))\n",
        "plt.plot(np.arange(1,num_of_x_values+1), plot_data['loss'])\n",
        "plt.plot(np.arange(1,num_of_x_values+1), plot_data['val_loss'])\n",
        "plt.scatter(np.arange(10,num_of_x_values+1, 10), history_test['test_loss'], s=50, color='green')\n",
        "plt.scatter([best_epic_number], scores[0], s=50, color='green', marker='x')\n",
        "\n",
        "plt.title('Loss per Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('Epoch')\n",
        "#plt.xticks(np.arange(1,len(history.epoch)+1))\n",
        "plt.xticks(np.arange(0,num_of_x_values+1,10))\n",
        "plt.legend(['train', 'validation', 'test', 'best'], loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjf-l0PK8jW3",
        "outputId": "0963ece2-f5c8-4e6f-d5c3-60096c7be610"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 299, 299, 3)]     0         \n",
            "                                                                 \n",
            " vgg19 (Functional)          (None, 9, 9, 512)         20024384  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              525312    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 1024)             4096      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 1024)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 102)               104550    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,658,342\n",
            "Trainable params: 631,910\n",
            "Non-trainable params: 20,026,432\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4gkG0dRPJ1a"
      },
      "source": [
        "#Extracting the 102Flowers dataset labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "GPLs82D3NkXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "879b2782-699e-4fe4-e0cf-74093920d453"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "502"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "URL = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\"\n",
        "\n",
        "# 1. Import the requests library\n",
        "import requests\n",
        "\n",
        "# 2. download the data behind the URL\n",
        "response = requests.get(URL)\n",
        "# 3. Open the response into a new file called instagram.ico\n",
        "open(\"my_lables.mat\", \"wb\").write(response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "8Zg_dhvzORcn"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "mat = scipy.io.loadmat('my_lables.mat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "AWSVmAo0OVjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28e3e7fb-6a9d-4436-8974-725b4d958134"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'__globals__': [],\n",
              " '__header__': b'MATLAB 5.0 MAT-file, Platform: GLNX86, Created on: Thu Feb 19 15:43:33 2009',\n",
              " '__version__': '1.0',\n",
              " 'labels': array([[77, 77, 77, ..., 62, 62, 62]], dtype=uint8)}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYwqp8aDp4Dd"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(mat['labels'].T)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "0rIjiDvJObef"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(mat['labels'].T)\n",
        "df.to_csv('102Flowers_labels.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "F4gkG0dRPJ1a"
      ],
      "machine_shape": "hm",
      "name": "CNN-FINAL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxe4+jJI038lXGJPehmRGh"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}